##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline template
===========================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

Changelog (Rerun any data before these dates):


-10/04/2018: pipeline_atacseq_encode has been modified (call_peaks_broad and call_peaks_narrow 
decorators modified to generate all the files generated by calling the peaks (not only the .xls files).
This pipeline has been adapted to the same changes and tested.


Overview
========

This pipeline starts with samples filtered, shifted single ends resulting from 
pipeline_atacseq_encode.filterShiftTagAlign.
1) It counts the number of SE in each sample (count_sample_filtered_shifted_SE)
2) It gets the minimum SE counts of all samples (get_min_count_sample_filtered_shifted_SE)
3) For each sample, it extracts a random sample with the minimum SE counts of all samples (get_balanced_sample_filtered_shifted_SE)
4) It pools the SE from 3) together (pool_all_balanced_samples_filtered_shifted_SE)
5) From here onwards it performs the same sequence as the pipeline_atacseq_encode and routine checks should be performed to make sure this is kept up to date here:
    -It calls the peaks broad and narrow (call_peaks_broad, call_peaks_narrow)
    -It filters peaks for regions of low and high mappability (regions specified by parameter): filter_peaks
    -It merges all the peaks and spaces within a certain distance: (merge_all_peaks)  

 
in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_atac_consensus_balanced_peaks.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------
NOTE: That it overrides directories and files which can also be generated by 
pipeline_atacseq_encode. Keep different runs in different directories.

NOTE: When combining data from different pipeline_atacseq_encode runs, make sure the runs are compatible
in terms of the peak calling and mappability filtering attributes.

The same requirements as for pipeline_atacseq_encode.

It performs the same sequence as the pipeline_atacseq_encode:
call_peaks_broad, call_peaks_narrow -> merge_all_peaks
and routine checks should be performed to make sure this is kept up to date here.


On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
sys.path.insert(0, "/home/mbp15ja/dev/pipeline_atacseq/src/")
import pipeline_atacseq_encode
sys.path.insert(0, "/home/mbp15ja/dev/AuxiliaryPrograms/logParsers/")
import logParser
import CGAT.IOTools as IOTools
import tempfile
import random
import pipelineAtacConsensusBalancedPeaks


# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


# ---------------------------------------------------
# Specific pipeline tasks
@follows(mkdir("filtered_tag_align_count_balanced.dir"))
@transform("filtered_tag_align.dir/*.single.end.shifted.filtered.tagAlign.gz",
           regex("filtered_tag_align.dir/(.+).single.end.shifted.filtered.tagAlign.gz"),
           r"filtered_tag_align_count_balanced.dir/\1.tsv")
def count_sample_filtered_shifted_SE(infile, outfile):
    ''' Counts the sample shifted, filtered SE from the corresponding folder of the atacseq
    pipeline task filterShiftTagAlign'''
    
    # Reuse the method in the atac-seq pipeline
    # The temp directory is gotten from the pipeline.ini which has the same attribute name
    pipeline_atacseq_encode.calculateNumberOfSingleEnds(infile, outfile)
    


@follows(mkdir("min_sample_SE_count_balanced.dir"))
@merge(count_sample_filtered_shifted_SE,
       "min_sample_SE_count_balanced.dir/subsampling_count.tsv")
def get_min_count_sample_filtered_shifted_SE(infiles, outfile):
    ''' Gets all the SE counts and obtains the minimum '''
    
    # Preallocate a list of length number of infiles
    list_counts = range(0,len(infiles))
    
    pos_counter = 0
    
    for infile in infiles:
        
        # Read each count into the file
        list_counts[pos_counter] = logParser.readSingleNumberFile(infile)
        
        pos_counter += 1
        
    # Get the minimum converting the list to int first
    min_value = min(map(int, list_counts))
    
    # Output to file
    with IOTools.openFile(outfile, "w") as writer:
        
        writer.write(str(min_value))
        
    writer.close()
    
    

# Based on pipeline_genomic_sample_bootstrap.generateBootstrapSamplesPerChromosome
@follows(mkdir("filtered_tag_align_balanced.dir"),
         get_min_count_sample_filtered_shifted_SE)
@transform("filtered_tag_align.dir/*.single.end.shifted.filtered.tagAlign.gz",
           regex("filtered_tag_align.dir/(.+).single.end.shifted.filtered.tagAlign.gz"),
           add_inputs(get_min_count_sample_filtered_shifted_SE),
           r"filtered_tag_align_balanced.dir/\1.single.end.shifted.filtered.tagAlign.gz")
def get_balanced_sample_filtered_shifted_SE(infiles, outfile):
    ''' Samples a random sample of reads from each sample. Each sample contains the same
    number of reads: the minimum sample single ends. Returns a file with the same sorting
    as the input'''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    temp_outfile = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    temp_extract_bed = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    

    # Separate the infiles
    bed_tags, read_count_file = infiles
    
    # Get the number of reads to sample
    number_output_reads = logParser.readSingleNumberFile(read_count_file)
    
    # Set the replacement flag to Sample without replacement
    replacement = "-o"
     
    # Generate a random positive number for the seed between 1-100
    random_number = random.randrange(1, 101)
    
    # Extract the file and generate the sample
    statement = '''zcat %(bed_tags)s > %(temp_extract_bed)s; 
                    checkpoint;
                    
                    sample %(replacement)s --preserve-order -d %(random_number)s -k %(number_output_reads)s %(temp_extract_bed)s | gzip > %(temp_outfile)s;
                    checkpoint;
                    
                    mv %(temp_outfile)s %(outfile)s;
                    rm %(temp_extract_bed)s
                    '''
    
    # Samples with number of single ends > 100M
    job_memory = "8G"
    
    P.run()
    

@follows(mkdir("filtered_tag_align_pool_balanced.dir"))
@merge(get_balanced_sample_filtered_shifted_SE,
       "filtered_tag_align_pool_balanced.dir/pooled.single.end.shifted.filtered.tagAlign.gz")    
def pool_all_balanced_samples_filtered_shifted_SE(infiles, outfile):    
    ''' Pools all the equal numbers of SE from all the samples together '''
    
    # Get the temporal dir specified
    tmp_dir = PARAMS["general_temporal_dir"]
    
    # Create temp files to make sure the process doesn't create the files
    # until it is finished 
    outfile_temp_pooled = (tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False)).name
    
    # The samples_table contains the name of the samples, but a suffix
    # has to be added to get from samples to shifted tag files
    file_suffix = ".PE2SE.tn5_shifted.tagAlign.gz"
    
    # Get all the samples pooled
    statement = pipelineAtacConsensusBalancedPeaks.SEPooling(infiles, outfile_temp_pooled)
    
    P.run()
    
    # Now move the files to the appropiate directory
    # Touch temp files in case they don't exist
    statement = ''' touch %(outfile_temp_pooled)s;
                    
                    mv %(outfile_temp_pooled)s %(outfile)s;
                    '''
    
    P.run()
    
    
    

# Using pipeline_atacseq_encode.call_peaks_broad
@follows(mkdir("peaks_broad.dir"))
@transform(pool_all_balanced_samples_filtered_shifted_SE,
           formatter("filtered_tag_align_pool_balanced.dir/pooled.single.end.shifted.filtered.tagAlign.gz"),
           ["peaks_broad.dir/pooled_peaks.broadPeak.gz",
            "peaks_broad.dir/pooled_peaks.gappedPeak.gz",
            "peaks_broad.dir/pooled_peaks.xls"],
           "pooled")
def call_peaks_broad(infile, outfiles, sample):
    ''' Use MACS2 to calculate peaks '''
    
    # The variables:
    # -macs2_threshold_method
    # -macs2_threshold_quantity
    # -end_extending_shift
    # -end_extending_extsize
    # -general_temporal_dir
    # are gotten from from the pipeline.ini which has the same attribute name
    pipeline_atacseq_encode.call_peaks_broad(infile, outfiles, sample)
    
    

# Using pipeline_atacseq_encode.call_peaks_narrow
@follows(mkdir("peaks_narrow.dir"))
@transform(pool_all_balanced_samples_filtered_shifted_SE,
           formatter("filtered_tag_align_pool_balanced.dir/pooled.single.end.shifted.filtered.tagAlign.gz"),
           ["peaks_narrow.dir/pooled_peaks.narrowPeak.gz",
            "peaks_narrow.dir/pooled_control_lambda.bdg",
            "peaks_narrow.dir/pooled_peaks.xls",
            "peaks_narrow.dir/pooled_summits.bed",
            "peaks_narrow.dir/pooled_treat_pileup.bdg"],
           "pooled")
def call_peaks_narrow(infile, outfiles, sample):
    ''' Use MACS2 to calculate peaks '''
    
    # The variables:
    # -macs2_threshold_method
    # -macs2_threshold_quantity
    # -end_extending_shift
    # -end_extending_extsize
    # -general_temporal_dir
    # are gotten from from the pipeline.ini which has the same attribute name
    pipeline_atacseq_encode.call_peaks_narrow(infile, outfiles, sample)




@follows(call_peaks_broad,
         call_peaks_narrow)
def call_peaks():
    ''' Dummy task to sync the call of peaks '''
    pass    


@follows(mkdir("filtered_peaks.dir"),
         call_peaks)
@transform(["peaks_narrow.dir/*_peaks.*.gz", "peaks_broad.dir/*_peaks.*.gz"],
           formatter(".+/(?P<SAMPLE>.+)\.(?P<EXTENSION>(narrowPeak|gappedPeak|broadPeak))\.gz"),
           "filtered_peaks.dir/{SAMPLE[0]}.{EXTENSION[0]}.gz")
def filter_peaks(infile, outfile):
    ''' Filters out regions of low mappability and excessive mappability '''
    
    # The variables:
    # -filtering_bed_exclusions
    # are gotten from from the pipeline.ini which has the same attribute name
    pipeline_atacseq_encode.filter_peaks(infile, outfile)



@follows(mkdir("common_peaks.dir"))
@collate(filter_peaks,
        regex(r".+\.dir/.+_peaks\.(narrowPeak|broadPeak)\.gz$"),
        "common_peaks.dir/all_merged_narrow_broad_peaks.gz")
def merge_all_peaks(infiles, outfile):
    ''' Merges all the narrow peaks and broad peaks produced by all
    the samples which are within a specified merging range.
    Outputs a coordinate sorted file'''
    
    # The variables:
    # -general_temporal_dir
    # are gotten from from the pipeline.ini which has the same attribute name
    # For now the merging distance is hardcorded
    pipeline_atacseq_encode.merge_all_peaks(infiles, outfile)
    


# ---------------------------------------------------
# Generic pipeline tasks
@follows(merge_all_peaks)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
